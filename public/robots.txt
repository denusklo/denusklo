# robots.txt - tells search engine crawlers what they can/cannot access
# Learn more: https://developers.google.com/search/docs/crawling-indexing/robots-txt

User-agent: *
Allow: /

# Disallow access to API endpoints or private areas (if any)
# Disallow: /api/
# Disallow: /private/

# Sitemap location - helps crawlers find all your pages
Sitemap: https://www.denusklo.com/sitemap-index.xml
